{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from os.path import join, basename, exists\n",
    "from os import makedirs\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from nilearn import input_data, datasets, plotting, regions\n",
    "from nilearn.image import concat_imgs\n",
    "from nilearn.input_data import NiftiLabelsMasker\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import nipype.pipeline.engine as pe\n",
    "import nipype.interfaces.io as nio\n",
    "import nipype.interfaces.utility as util\n",
    "from nipype.interfaces.fsl import InvWarp\n",
    "\n",
    "import bct\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing \n",
    "Largely following the Westphal et al. (2017) paper, but taking into account the things that Dani Bassett does in her papers (which I still need to look into).\n",
    "### Preprocessing methods per Westphal et al., 2017\n",
    "Whole-brain MRI was administered on a 3.0 Tesla Siemens TIM Trio scanner at the UCLA Staglin Center for Cognitive Neuroscience. Functional images were ac- quired using a T2*-weighted echoplanar imaging sequence (TR   2.0 s; TE   30 ms; flip angle   75Â°; FOV   19.2 cm; voxel resolution   3.0   3.0   3.7 mm; 33 interleaved axial slices). The first three volumes of each 239-volume run were discarded to ensure T1 stabilization.<br>Preprocessing was done in SPM8.\n",
    "1. Slice timing correction\n",
    "2. Motion correction\n",
    "3. Unwarping\n",
    "4. Coregistration to subject's T1\n",
    "5. Anatomical segmentation\n",
    "6. Spatial normalization to MNI template\n",
    "7. Spatial smoothing (6mm FWHM)\n",
    "8. High-pass filtering (236_s_)\n",
    "9. Timecourse per voxel demeaned\n",
    "<br>\n",
    "### Alterations made below\n",
    "Preprocessing was done with FSL tools in Nipype.\n",
    "3. No fieldmaps, so no unwarping... (look into this)\n",
    "7. No smoothing\n",
    "8. High pass filtering (??)\n",
    "9. Demeaning???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preproc(data_dir, sink_dir, subject, run, highpass, masks, mask_names):\n",
    "    from nipype.interfaces.fsl import MCFLIRT, FLIRT, FNIRT, ExtractROI, ApplyWarp, MotionOutliers\n",
    "    from nilearn.plotting import plot_roi\n",
    "    from nilearn import input_data\n",
    "    \n",
    "    #WRITE A DARA GRABBER\n",
    "    def get_niftis(subject_id, data_dir, run):\n",
    "        from os.path import join, exists\n",
    "        t1 = join(data_dir, subject_id, 'session-1', 'anatomical', 'anatomical-0', 'anatomical.nii.gz')\n",
    "        #t1_brain_mask = join(data_dir, subject, 'session-1', 'anatomical', 'anatomical-0', 'fsl', 'anatomical-bet.nii.gz')\n",
    "        epi = join(data_dir, subject_id, 'session-1', 'retr', 'retr-{0}'.format(run), 'retr.nii.gz')\n",
    "        assert exists(t1), \"t1 does not exist\"\n",
    "        assert exists(epi), \"epi does not exist\"\n",
    "        standard = '/home/applications/fsl/5.0.8/data/standard/MNI152_T1_2mm.nii.gz'\n",
    "        return t1, epi, standard\n",
    "    \n",
    "    data = Function(function=get_niftis, input_names=[\"subject_id\", \"data_dir\", \"run\"],\n",
    "                            output_names=[\"t1\", \"bold\", \"standard\"])\n",
    "    data.inputs.data_dir = data_dir\n",
    "    data.inputs.subject_id = subject\n",
    "    data.inputs.run = run\n",
    "    \n",
    "    grabber = data.run()\n",
    "    \n",
    "    standard = '/home/applications/fsl/5.0.8/data/standard/MNI152_T1_2mm_brain.nii.gz'\n",
    "    qa1 = join(sink_dir, 'qa', '{0}_mni_flirt_{1}.png'.format(subject, run))\n",
    "    qa2 = join(sink_dir, 'qa', '{0}_mni_fnirt_{1}.png'.format(subject, run))\n",
    "    confound_file = join(data_dir, subject, 'session-1', 'retr', 'mni', 'confounds-{0}.csv'.format(run))\n",
    "    \n",
    "    #run motion correction\n",
    "    mcflirt = MCFLIRT(ref_vol=144, save_plots=True, output_type='NIFTI_GZ')\n",
    "    mcflirt.inputs.in_file = join(data_dir, subject, 'session-1', 'retr', 'retr-{0}'.format(run), 'retr.nii.gz')\n",
    "    mcflirt.inputs.out_file = join(data_dir, subject,'session-1', 'retr', 'mni', 'mcf-func-{0}.nii.gz'.format(run))\n",
    "    flirty = mcflirt.run()\n",
    "    \n",
    "    #motion outliers\n",
    "    mout = MotionOutliers(no_motion_correction=True, metric='fd')\n",
    "    mout.inputs.in_file = flirty.outputs.out_file\n",
    "    moutliers = mout.run()\n",
    "    \n",
    "    #concatenate motion parameters and motion outliers to form confounds file\n",
    "    motion = np.genfromtxt(flirty.outputs.par_file)\n",
    "    outliers = np.genfromtxt(moutliers.outputs.out_file)\n",
    "    conf = np.hstack((motion, outliers))\n",
    "    np.savetxt(confound_file, conf, delimiter=',')\n",
    "    \n",
    "    #extract an example volume for normalization\n",
    "    ex_fun = ExtractROI(t_min=144, t_size=1)\n",
    "    ex_fun.inputs.in_file = flirty.outputs.out_file\n",
    "    ex_fun.outputs.roi_file = join(data_dir, subject,'session-1', 'retr', 'mni', 'example-func-{0}.nii.gz'.format(run))\n",
    "    fun = ex_fun.run()\n",
    "    \n",
    "    #two-step normalization with flirt and fnirt, outputting qa pix\n",
    "    flit = FLIRT(cost_func=\"corratio\", dof=7)\n",
    "    flit.inputs.reference = fun.outputs.roi_file\n",
    "    flit.inputs.in_file = grabber.outputs.t1\n",
    "    flit.inputs.out_file = join(data_dir, subject, 'session-1', 'retr', 'mni', 't1-flirt-func-{0}.nii.gz'.format(run))\n",
    "    flit.inputs.out_matrix_file = join(data_dir, subject, 'session-1', 'retr', 'mni', 't1-to-func-{0}.mat'.format(run))\n"
,
    "    reg1 = flit.run()\n",
    "    #plot flirted MNI overlaid on example_func\n",
    "    plot_roi(reg1.outputs.out_file, bg_img=fun.outputs.roi_file, colorbar=True, draw_cross=False, output_file=qa1)\n",
    "    \n",
    "    perf = FNIRT(output_type='NIFTI_GZ')\n",
    "    perf.inputs.warped_file = join(data_dir, subject, 'session-1', 'retr', 'mni', 'mni-fnirt-func-{0}.nii.gz'.format(run))\n",
    "    perf.inputs.affine_file = reg1.outputs.out_matrix_file\n",
    "    perf.inputs.in_file = standard\n",
    "    perf.inputs.ref_file = fun.outputs.roi_file\n",
    "    reg2 = perf.run()\n",
    "    #plot fnirted MNI overlaid on example func\n",
    "    plot_roi(reg2.outputs.warped_file, bg_img=fun.outputs.roi_file, colorbar=True, draw_cross=False, output_file=qa2)\n",
    "    \n",
    "    xfmd_ntwks = []\n",
    "    for i in np.arange(0, len(masks)):\n",
    "        warp = ApplyWarp(interp=\"nn\", abswarp=True)\n",
    "        warp.inputs.in_file = masks[i]\n",
    "        warp.inputs.ref_file = fun.outputs.roi_file\n",
    "        warp.inputs.field_file = reg2.outputs.field_file\n",
    "        net_warp = warp.run()\n",
    "        xfmd_ntwks.append(net_warp.outputs.out_file)\n",
    "        \n",
    "        qa_file = join(sink_dir, 'qa', '{0}_ntwk_qa_{1}-{2}.png'.format(subject, mask_names[i], run))\n",
    "\n",
    "        plotting.plot_roi(net_warp.outputs.out_file, bg_img=fun.outputs.roi_file, \n",
    "                          colorbar=True, vmin=0, vmax=18, draw_cross=False, output_file=qa_file)\n",
    "    \n",
    "    ntwk_masker = input_data.NiftiLabelsMasker(xfmd_ntwks[0], standardize=True, high_pass=highpass, t_r=2.0)\n",
    "    hippo_masker = input_data.NiftiLabelsMasker(xfmd_ntwks[1], standardize=True, high_pass=highpass, t_r=2.0)\n",
    "    \n",
    "    return flirty.outputs.out_file, confound_file, ntwk_masker, hippo_masker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fnirt --aff=/Users/Katie/Dropbox/Data/hypothalamus/HIP007/H-tract-2016/struc2diff.mat --in=/Users/Katie/Dropbox/Data/templates/Colin27_T1_seg_MNI-152.nii.gz --logout=/Users/Katie/Dropbox/Projects/physics-retrieval/scripts/idconn-retrieval/Colin27_T1_seg_MNI-152_log.txt --ref=/Users/Katie/Dropbox/Data/hypothalamus/HIP007/H-tract-2016/nodif.nii.gz --iout=/Users/Katie/Dropbox/Data/hypothalamus/HIP007/H-tract-2016/struc2diff.nii.gz\n",
      "applywarp --in=/Users/Katie/Dropbox/Projects/physics-retrieval/18-networks-5.14.nii.gz --ref=/Users/Katie/Dropbox/Data/hypothalamus/HIP007/H-tract-2016/nodif.nii.gz --out=/Users/Katie/Dropbox/Projects/physics-retrieval/scripts/idconn-retrieval/18-networks-5.14_warp.nii.gz --warp=/Users/Katie/Dropbox/Data/habenula/output/HIP002/reg/HIP002_anat2std_warp.nii.gz --interp=nn\n"
     ]
    }
   ],
   "source": [
    "from nipype.interfaces.fsl import MCFLIRT, FLIRT, FNIRT, ExtractROI, ApplyWarp, MotionOutliers\n",
    "standard = '/Users/Katie/Dropbox/Data/templates/Colin27_T1_seg_MNI-152.nii.gz'\n",
    "perf = FNIRT(output_type='NIFTI_GZ')\n",
    "perf.inputs.warped_file = '/Users/Katie/Dropbox/Data/hypothalamus/HIP007/H-tract-2016/struc2diff.nii.gz'\n",
    "perf.inputs.affine_file = '/Users/Katie/Dropbox/Data/hypothalamus/HIP007/H-tract-2016/struc2diff.mat'\n",
    "perf.inputs.in_file = standard\n",
    "perf.inputs.ref_file = '/Users/Katie/Dropbox/Data/hypothalamus/HIP007/H-tract-2016/nodif.nii.gz'\n",
    "#perf.inputs.fieldcoeff_file = join(data_dir, '101', 'session-1', 'retr', 'mni', 'mni-fnirt-func-1-coeff.nii.gz')\n",
    "print perf.cmdline\n",
    "\n",
    "warp = ApplyWarp(interp='nn')\n",
    "warp.inputs.in_file = masks[0]\n",
    "warp.inputs.ref_file = '/Users/Katie/Dropbox/Data/hypothalamus/HIP007/H-tract-2016/nodif.nii.gz'\n",
    "warp.inputs.field_file = '/Users/Katie/Dropbox/Data/habenula/output/HIP002/reg/HIP002_anat2std_warp.nii.gz'\n",
    "print warp.cmdline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#data grabbing function\n",
    "def get_niftis(data_dir, subject_id, session, task):\n",
    "    from os.path import join, exists\n",
    "    t1 = join(data_dir, subject_id, 'T1W', 'anat.nii.gz')\n",
    "    epi1 = join(data_dir, subject_id, 'session-{0}'.format(session-1), task, '{0}-0'.format(task), '{0}-5mm.feat'.format(task), 'filtered_func_data.nii.gz')\n",
    "    epi2 = join(data_dir, subject_id, 'session-{0}'.format(session-1), task, '{0}-1'.format(task), '{0}-5mm.feat'.format(task), 'filtered_func_data.nii.gz')\n",
    "\n",
    "    assert exists(t1), \"t1 does not exist\"\n",
    "    assert exists(epi1), \"epi does not exist\"\n",
    "    assert exists(epi2), \"epi does not exist\"\n",
    "    standard = '/home/applications/fsl/5.0.8/data/standard/MNI152_T1_2mm_brain.nii.gz'\n",
    "    return t1, epi1, epi2, standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mni_to_epi(data_dir, sink_dir, subject, run, masks, mask_names):\n",
    "    from nipype.interfaces.fsl import InvWarp, ApplyWarp\n",
    "    from nilearn import plotting\n",
    "    \n",
    "    example_func = join(data_dir, subject, 'session-1', 'retr', 'retr-{0}'.format(run), 'retr-5mm.feat', 'reg', 'example_func.nii.gz')\n",
    "    func_to_mni = join(data_dir, subject, 'session-1', 'retr', 'retr-{0}'.format(run), 'retr-5mm.feat', 'reg', 'example_func2standard_warp.nii.gz')\n",
    "    \n",
    "    \n",
    "    invwarp = InvWarp(output_type=\"NIFTI_GZ\")\n",
    "    invwarp.inputs.warp = func_to_mni\n",
    "    invwarp.inputs.reference = example_func\n",
    "    mni_warp = invwarp.run()\n",
    "    \n",
    "    xfmd_ntwks = []\n",
    "    \n",
    "    for i in np.arange(0, len(masks)):\n",
    "        applywarp = ApplyWarp(interp=\"nn\", abswarp=True)\n",
    "        applywarp.inputs.in_file = masks[i]\n",
    "        applywarp.inputs.ref_file = join(data_dir, subject, 'session-1', 'retr', 'retr-{0}'.format(run), 'retr-5mm.feat', 'reg', 'example_func.nii.gz')\n",
    "        applywarp.inputs.field_file = mni_warp.outputs.inverse_warp\n",
    "        network_warp = applywarp.run()\n",
    "        xfmd_ntwks.append(network_warp.outputs.out_file) \n",
    "        \n",
    "        qa_file = join(sink_dir, 'qa', '{0}_ntwk_qa_{1}.png'.format(subject, mask_names[i], run))\n",
    "\n",
    "        plotting.plot_roi(xfmd_ntwks, bg_img=example_func, cut_coords=(-2, 5, 40), \n",
    "                          colorbar=True, vmin=0, vmax=18, draw_cross=False, \n",
    "                          output_file=qa_file)\n",
    "    \n",
    "    return xfmd_ntwks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#choose your atlas and either fetch it from Nilearn using one of the the 'datasets' functions\n",
    "laird_2011_icns = '/home/data/nbc/physics-learning/retrieval-graphtheory/18-networks-5.14.nii.gz'\n",
    "#laird_2011_icns = '/Users/Katie/Dropbox/Projects/physics-retrieval/18-networks-5.14.nii.gz'\n",
    "#add labels for the parcellation\n",
    "labels = ['limbic', 'orbitofrontal', 'basal ganglia', 'salience', 'hunger', \n",
    "          'motor learning', 'frontoparietal', 'hand', 'motor execution', 'higher order visual', \n",
    "          'lateral visual', 'medial visual', 'default mode',' cerebellum', 'left central executive', \n",
    "          'auditory', 'mouth', 'right central executive']\n",
    "\n",
    "harvox_hippo = '/home/data/nbc/physics-learning/retrieval-graphtheory/harvox-hippo-prob50-2mm.nii.gz'\n",
    "masks = [laird_2011_icns, harvox_hippo]\n",
    "mask_names = ['18_icn', 'hippo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_roi(harvox_hippo, cut_coords=(-25, -10, -6), colorbar=True, vmin=0, vmax=2, draw_cross=False)\n",
    "plotting.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotting.plot_roi(laird_2011_icns, cut_coords=(-20, -10, 0, 10, 20, 30, 40, 50, 60, 70), \n",
    "                  display_mode='z', colorbar=True, vmin=0, vmax=20, draw_cross=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create your masker objects \n",
    "#(this is how to pull timeseries from niftis into arrays, based on your parcellations scheme)\n",
    "#one masker to pull out one signal per network (average of voxelwise signals across the network)\n",
    "#high and low-pass options are in Hertz\n",
    "\n",
    "network_masker = input_data.NiftiLabelsMasker(laird_2011_icns, standardize=True, high_pass=)\n",
    "hippo_masker = input_data.NiftiLabelsMasker(harvox_hippo, standardize=True)\n",
    "#one masker to pull out one signal per region (if applicable)\n",
    "#region_masker = input_data.NiftiLabelsMasker('/Users/Katie/Dropbox/Data/salience-anxiety-graph-theory/relabeled_yeo_atlas7_gt100.nii.gz', standardize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#only want post subjects\n",
    "subjects = ['101', '102', '103', '104', '106', '107', '108', '110', '212',\n",
    "            '214', '215', '216', '217', '218', '219', '320', '321', '323',\n",
    "            '324', '325', '327', '328', '330', '331', '333', '334',\n",
    "            '335', '336', '337', '338', '339', '340', '341', '342', '343', '344',\n",
    "            '345', '346', '347', '348', '349', '350', '451', '453', '455',\n",
    "            '458', '459', '460', '462', '463', '464', '465', '467',\n",
    "            '468', '469', '470', '502', '503', '571', '572', '573', '574',\n",
    "            '577', '578', '581', '582', '584', '585', '586', '587',\n",
    "            '588', '589', '591', '592', '593', '594', '595', '596', '597',\n",
    "            '598', '604', '605', '606', '607', '608', '609', '610', '612',\n",
    "            '613', '614', '615', '617', '618', '619', '620', '621', '622',\n",
    "            '623', '624', '625', '626', '627', '629', '630', '631', '633',\n",
    "            '634']\n",
    "#all subjects 102 103 101 104 106 107 108 110 212 X213 214 215 216 217 218 219 320 321 X322 323 324 325 \n",
    "#327 328 X329 330 331 X332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 451 \n",
    "#X452 453 455 X456 X457 458 459 460 462 463 464 465 467 468 469 470 502 503 571 572 573 574 X575 577 578 \n",
    "#X579 X580 581 582 584 585 586 587 588 589 X590 591 592 593 594 595 596 597 598 604 605 606 607 608 609 \n",
    "#610 X611 612 613 614 615 X616 617 618 619 620 621 622 623 624 625 626 627 X628 629 630 631 633 634\n",
    "#errors in fnirt-to-mni: 213, 322, 329, 332, 452, 456, 457, 575, 579, 580, 590, 611, 616, 628\n",
    "#subjects without post-IQ measure: 452, 461, 501, 575, 576, 579, 583, 611, 616, 628, 105, 109, 211, 213, 322, 326, 329, 332\n",
    "#subjects = ['101', '102']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = '/home/data/nbc/physics-learning/data/pre-processed'\n",
    "sink_dir = '/home/data/nbc/physics-learning/retrieval-graphtheory/output'\n",
    "#sink_dir = '/Users/Katie/Dropbox/Projects/physics-retrieval/data/out'\n",
    "\n",
    "runs = [0, 1]\n",
    "connectivity_metric = 'correlation'\n",
    "networks = [12, 14, 17]\n",
    "conditions = ['phy', 'gen']\n",
    "thresh_range = np.arange(0.1, 1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#gen_timing = np.genfromtxt('/home/data/nbc/physics-learning/physics-learning/RETRconditionGeneralSess1.txt', \n",
    "#                           delimiter='\\t')\n",
    "gen_timing = np.genfromtxt('/home/data/nbc/physics-learning/retrieval-graphtheory/RETRconditionGeneralSess1.txt', \n",
    "                           delimiter='\\t', dtype=int)\n",
    "\n",
    "gen_timing = (gen_timing/2)-1\n",
    "gen_timing = gen_timing[:,0:2]\n",
    "\n",
    "#phy_timing = np.genfromtxt('/home/data/nbc/physics-learning/physics-learning/RETRconditionPhysicsSess1.txt', \n",
    "#                           delimiter='\\t')\n",
    "phy_timing = np.genfromtxt('/home/data/nbc/physics-learning/retrieval-graphtheory/RETRconditionPhysicsSess1.txt', \n",
    "                           delimiter='\\t')\n",
    "phy_timing = (phy_timing/2)-1\n",
    "phy_timing = phy_timing[:,0:2]\n",
    "timing = {}\n",
    "timing['phy'] = phy_timing\n",
    "timing['gen'] = gen_timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'global efficiency phy': np.empty([len(subjects),], dtype=float),\n",
    "                   'fc default mode-left central executive phy': np.empty([len(subjects),], dtype=float),\n",
    "                   'fc default mode-right central executive phy': np.empty([len(subjects),], dtype=float),\n",
    "                   'fc left central executive-right central executive phy': np.empty([len(subjects),], dtype=float),\n",
    "                   'fc hippo-default mode phy': np.empty([len(subjects),], dtype=float),\n",
    "                   'fc hippo-left central executive phy': np.empty([len(subjects),], dtype=float),\n",
    "                   'fc hippo-right central executive phy': np.empty([len(subjects),], dtype=float),\n",
    "                   'le default mode phy': np.empty([len(subjects),], dtype=float),\n",
    "                   'le right central executive phy': np.empty([len(subjects),], dtype=float),\n",
    "                   'le left central executive phy': np.empty([len(subjects),], dtype=float),\n",
    "                   'global efficiency gen': np.empty([len(subjects),], dtype=float),\n",
    "                   'fc default mode-left central executive gen': np.empty([len(subjects),], dtype=float),\n",
    "                   'fc default mode-right central executive gen': np.empty([len(subjects),], dtype=float),\n",
    "                   'fc left central executive-right central executive gen': np.empty([len(subjects),], dtype=float),\n",
    "                   'fc hippo-default mode gen': np.empty([len(subjects),], dtype=float),\n",
    "                   'fc hippo-left central executive gen': np.empty([len(subjects),], dtype=float),\n",
    "                   'fc hippo-right central executive gen': np.empty([len(subjects),], dtype=float),\n",
    "                   'le default mode gen': np.empty([len(subjects),], dtype=float),\n",
    "                   'le right central executive gen': np.empty([len(subjects),], dtype=float),\n",
    "                   'le left central executive gen': np.empty([len(subjects),], dtype=float)},\n",
    "                  index=subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for subject in subjects:\n",
    "    #print subject\n",
    "    if not exists(join(sink_dir, subject)):\n",
    "            makedirs(join(sink_dir, subject))\n",
    "    timeseries = {}\n",
    "    corrmats = {}\n",
    "    hippo = {}\n",
    "    for condition in conditions:\n",
    "        for i in np.arange(0, len(runs)):\n",
    "            example_func = \n",
    "            #xfm laird 2011 maps to subject's epi space & define masker\n",
    "            epi_masks = preproc(data_dir, sink_dir, subject, i, highpass, masks, mask_names)\n",
    "            network_masker = input_data.NiftiLabelsMasker(epi_masks[0], standardize=True)\n",
    "            hippo_masker = input_data.NiftiLabelsMasker(epi_masks[1], standardize=True)\n",
    "            plotting.plot_roi(laird_2011_icns, bg_img=example_func, cut_coords=(-2, 5, 40), \n",
    "                          colorbar=True, vmin=0, vmax=18, draw_cross=False, \n",
    "                          output_file=qa_file)\n",
    "            \n",
    "            \n",
    "            #extract network-wise timeseries from nifti into array\n",
    "            run = join(data_dir, subject, 'session-1', 'retr', 'mni', '{0}_filtered_func_data_{1}.nii.gz'.format(subject, i))\n",
    "            ts = network_masker.fit_transform(run)\n",
    "            hippo_ts = hippo_masker.fit_transform(run)\n",
    "\n",
    "            #separate into different conditions\n",
    "            timeseries['{0} {1}'.format(condition, i)] = np.vstack((ts[timing[condition][0,0].astype(int):(timing[condition][0,0]+timing[condition][0,1]+1).astype(int), :], ts[timing[condition][1,0].astype(int):(timing[condition][1,0]+timing[condition][1,1]+1).astype(int), :], ts[timing[condition][2,0].astype(int):(timing[condition][2,0]+timing[condition][2,1]+1).astype(int), :]))\n",
    "            \n",
    "            #FOR SOME REASON, THIS CAUSES THEM TO BE HSTACKED? IDK WHAT'S GOING ON\n",
    "            hippo['{0} {1}'.format(condition, i)] = np.vstack((hippo_ts[timing[condition][0,0].astype(int):(timing[condition][0,0]+timing[condition][0,1]+1).astype(int)], hippo_ts[timing[condition][1,0].astype(int):(timing[condition][1,0]+timing[condition][1,1]+1).astype(int)], hippo_ts[timing[condition][2,0].astype(int):(timing[condition][2,0]+timing[condition][2,1]+1).astype(int)]))\n",
    "            #print 'ts has shape {0}'.format(hippo['{0} {1}'.format(condition, i)].shape)\n",
    "        #splice runs together\n",
    "        timeseries[condition] = np.vstack((timeseries['{0} 0'.format(condition)], timeseries['{0} 1'.format(condition)]))\n",
    "        #print 'ts {0} = {1}'.format(condition, timeseries[condition].shape)\n",
    "\n",
    "        #compute correlation matrices per condition per run\n",
    "        correlation_measure = ConnectivityMeasure(kind=connectivity_metric)\n",
    "        corrmats[condition] = correlation_measure.fit_transform([timeseries[condition]])[0]\n",
    "        np.savetxt(join(sink_dir, subject, '{0}-{1}-corrmat.csv'.format(subject, condition)), corrmats[condition])\n",
    "    \n",
    "        #populate df with network FC measures\n",
    "        hippo[condition] = np.vstack((hippo['{0} 0'.format(condition)], hippo['{0} 1'.format(condition)]))\n",
    "        #print 'fc default mode-left central executive {0}'.format(condition), corrmats[condition][12,14]\n",
    "        df.at[subject, 'fc default mode-left central executive {0}'.format(condition)] = corrmats[condition][12,14]\n",
    "        df.at[subject, 'fc default mode-right central executive {0}'.format(condition)] = corrmats[condition][12,17]\n",
    "        df.at[subject, 'fc left central executive-right central executive {0}'.format(condition)] = corrmats[condition][14,17]\n",
    "        \n",
    "        #and since we're here, calculate graph theory measures & populate df\n",
    "        ge = []\n",
    "        le = {}\n",
    "        loceff = {}\n",
    "        loceff['default mode'] = []\n",
    "        loceff['left central executive'] = []\n",
    "        loceff['right central executive'] = []\n",
    "        for p in thresh_range:\n",
    "            corrmat_thresh = bct.threshold_proportional(corrmats[condition], p, copy=True)\n",
    "            #measures of interest here\n",
    "            #global efficiency\n",
    "            geff = bct.efficiency_wei(corrmat_thresh)\n",
    "            ge.append(geff)\n",
    "\n",
    "            #local efficiency\n",
    "            leff = bct.efficiency_wei(corrmat_thresh, local=True)\n",
    "            #print leff[2]\n",
    "            for network in networks:\n",
    "                #print network\n",
    "                loceff[labels[network]].append(leff[network])\n",
    "                #loceff['{0}, {1}'.format(labels[network], condition)].append(leff[network])\n",
    "            #print loceff\n",
    "            le['{0}, {1}'.format(p, condition)] = loceff\n",
    "        \n",
    "        #print 'global efficiency is {0}'.format(ge)\n",
    "        df.at[subject, 'global efficiency {0}'.format(condition)] = np.trapz(ge, dx=0.1)\n",
    "        \n",
    "        #populate df with hippo-FCs and local efficiencies per network\n",
    "        for network in networks:\n",
    "            df.at[subject, 'le {0} {1}'.format(labels[network], condition)] = np.trapz(loceff[labels[network]], dx=0.1)\n",
    "            #calculate hippocampus-network correlations (FC) per network & populate df \n",
    "            network_ts = np.reshape(timeseries[condition][:,network], (84,1))\n",
    "            #print(network_ts.shape)\n",
    "            df.at[subject, 'fc hippo-{0} {1}'.format(labels[network], condition)] = pearsonr(hippo[condition], network_ts)[0]\n",
    "df.to_csv(join(sink_dir, 'out.csv'), sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "local_dir = '/Users/Katie/Dropbox/Projects/physics-retrieval/data/out/'\n",
    "phy = np.zeros((18, 18))\n",
    "gen = np.zeros((18, 18))\n",
    "\n",
    "for subject in subjects:\n",
    "    phy += np.genfromtxt(join(local_dir, subject, '{0}-phy-corrmat.csv'.format(subject)), delimiter=' ')\n",
    "    gen += np.genfromtxt(join(local_dir, subject, '{0}-gen-corrmat.csv'.format(subject)), delimiter=' ')\n",
    "phy_corrmat = phy / len(subjects)\n",
    "gen_corrmat = gen / len(subjects)\n",
    "\n",
    "dif_corrmat = phy_corrmat - gen_corrmat\n",
    "\n",
    "phy_corr_df = pd.DataFrame(phy_corrmat, index=labels, columns=labels)\n",
    "gen_corr_df = pd.DataFrame(gen_corrmat, index=labels, columns=labels)\n",
    "dif_corr_df = pd.DataFrame(dif_corrmat, index=labels, columns=labels)\n",
    "\n",
    "#np.savetxt(join(local_dir, 'avg_phys_corrmat.csv'), phy_corrmat, delimiter=',')\n",
    "#np.savetxt(join(local_dir, 'avg_gen_corrmat.csv'), gen_corrmat, delimiter=',')\n",
    "\n",
    "phy_corr_df.to_csv(join(local_dir, 'avg_phys_corrmat.csv'), sep=',')\n",
    "gen_corr_df.to_csv(join(local_dir, 'avg_gen_corrmat.csv'), sep=',')\n",
    "dif_corr_df.to_csv(join(local_dir, 'avg_dif_corrmat.csv'), sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adj_to_list(input_filename,output_filename,delimiter):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import networkx as nx\n",
    "    import csv\n",
    "    '''https://github.com/jermainkaminski/Adjacency-Matrix-to-Edge-List/blob/master/adjacencymatrix_to_edgelist.ipynb'''\n",
    "    '''Takes the adjacency matrix on file input_filename into a list of edges and saves it into output_filename'''\n",
    "    A=pd.read_csv(input_filename,delimiter=delimiter,index_col=0)\n",
    "    List=[('Source','Target','Weight')]\n",
    "    for source in A.index.values:\n",
    "        for target in A.index.values:\n",
    "            List.append((target,source,A[source][target]))\n",
    "    with open(output_filename, \"wb\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(List)\n",
    "    return List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phy_list = adj_to_list(join(local_dir, 'avg_phys_corrmat.csv'), join(local_dir, 'avg_phys_edges.csv'), ',')\n",
    "gen_list = adj_to_list(join(local_dir, 'avg_gen_corrmat.csv'), join(local_dir, 'avg_gen_edges.csv'), ',')\n",
    "dif_list = adj_to_list(join(local_dir, 'avg_dif_corrmat.csv'), join(local_dir, 'avg_dif_edges.csv'), ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
